{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping CollegeRecruiter.com\n",
    "=============================\n",
    "This notebook goes through the process of scraping CollegeRecruiter to find out the distribution of jobs in a list of cities in the bay area using Python and BeautifulSoup\n",
    "\n",
    "Basic process flow:\n",
    "\n",
    "- Determine amount of jobs there are so you know how many pages to scrape in a loop\n",
    "- Create a loop to grab all the relevant job description links\n",
    "- Create a loop to go through each job page and look for city names in the HTML\n",
    "- Use a dictionary and counter to keep track of counts\n",
    "- Plot the distribution with a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Determine HTML Structure\n",
    "Pull HTML from a random page to parse in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.collegerecruiter.com/job-search?keyword=Analyst&location=San+Jose%2C+CA%2C+United+States&locMatches=1&lat=37.3382082&lng=-121.88632860000001&page=2\n"
     ]
    }
   ],
   "source": [
    "url = ('https://www.collegerecruiter.com/job-search?keyword=Analyst&location=San+Jose'\n",
    "      '%2C+CA%2C+United+States&locMatches=1&lat=37.3382082&lng=-121.88632860000001&page=2')\n",
    "print url\n",
    "# get HTML using urllib2 read()\n",
    "source = urllib2.urlopen(url).read()\n",
    "# Use BS4 to parse code\n",
    "bs_tree = bs4.BeautifulSoup(source, \"lxml\") #lxml given from warnings\n",
    "#print bs_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get total amount of jobs to scrape\n",
    "Inspect the page here: https://www.collegerecruiter.com/job-search?keyword=Analyst&location=San+Jose%2C+CA%2C+United+States&locMatches=1&lat=37.3382082&lng=-121.88632860000001&page=2\n",
    "\n",
    "The total amount of jobs is located in a div with class \"pull-left\", so parse for that div and scrape the string. Use digits mathematics to convert that string into a number. The method for that is explained here:\n",
    "\n",
    "The for loop and Zip() is used to pair each digit in the string with it's digits place by using len() (Ex: 25 has length of 2, so the '2' is in the 2nd digits place). Using that knowledge, convert the string by taking the sum of all digits when you multiply each digit by 10 ^ (digits place).\n",
    "\n",
    "Example: 142 = (2*(10^0))+(4*(10^1))+(1*(10^2)) = (2 + 40 + 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                        Jobs 11 -\r\n",
      "                        20 of 5199                    \n",
      "5199\n",
      "[5, 1, 9, 9]\n",
      "5199\n"
     ]
    }
   ],
   "source": [
    "#get total # of hits listed so we know what to loop to\n",
    "job_count_string = bs_tree.find(\"div\",class_= 'pull-left').contents[0]\n",
    "print job_count_string \n",
    "job_count_string = job_count_string.split()[-1]\n",
    "print job_count_string\n",
    "digits = []\n",
    "for i in job_count_string:\n",
    "    if i.isdigit():\n",
    "        digits.append(int(i))\n",
    "print digits #now convert the array to a number\n",
    "job_count = np.sum([digit*(10**exponent) for digit, exponent in \n",
    "                    zip(digits[::-1], range(len(digits)))])\n",
    "print job_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scrape and Compile All Job Links\n",
    "Divide the total amount of jobs by the amount of jobs shown per page to get the range limit for the loop to go through every page on the site. Create the first for loop to go through each page and find the HTML object with all the links. In this case, the tags are \"form\" and tags with class \"jobTitle\". Create another for loop to go through that HTML object and append all the links. \n",
    "\n",
    "Skip to next cell if you do not want to wait for the link scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Uncomment if you want to scrape all the links. Otherwise, use the pickle cell below to get the links.\n",
    "num_pages = int(np.ceil(job_count/10.0))\n",
    "\n",
    "job_links = []\n",
    "for i in range(1, job_count/10): #since pages have 10 jobs each, we will go from 1 - job_count/10 to get all job links\n",
    "    if i%10==0:\n",
    "        print num_pages-i\n",
    "    a = str(i)\n",
    "    url = \"https://www.collegerecruiter.com/job-search?keyword=Analyst&location=San+Jose\" \\\n",
    "          \"%2C+CA%2C+United+States&locMatches=1&lat=37.3382082&lng=-121.88632860000001&page=\" + a\n",
    "    #use loop to go through each url needed\n",
    "    html_page = urllib2.urlopen(url).read() \n",
    "    bs_tree = bs4.BeautifulSoup(html_page)\n",
    "    job_link_area = bs_tree.find_all(\"form\", method=\"POST\")[1] #list containing all form tags, only want 2nd one\n",
    "    job_postings = job_link_area.find_all(class_=\"jobTitle\") #jobTitle contains the link\n",
    "    for i in job_postings:\n",
    "        job_links.append(i.find(\"a\")['href']) #within the a tag in jobTitle, use href tag to access just the link\n",
    "\n",
    "    time.sleep(1) #delay between each scrape \n",
    "print \"We found a lot of jobs: \", len(job_links)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Pickling\n",
    "Scraping 520 pages takes a while, so I already scraped all of the job links and stored them in a Pickle object which is a file on GitHub called \"joblinks.pickle\". To load it you can run the following cell below. You can also see how to pickle by looking at the Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('joblinks.pickle','rb') as handle:\n",
    "    job_links = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Regex to get Counts\n",
    "Now that we have all the links of the jobs, loop through the list and get the HTML for each link. Then, use regex to delete all characters that are not a-z or \".\". Afterwards, loop through the HTML using each index in the cities dictionary, and add +1 to the dictionary if the city is found in the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities =  {\"sunnyvale\":0, \"san jose\": 0, \"campbell\":0, \"san francisco\": 0, \"santa clara\": 0,  \"mountain view\":0}\n",
    "for link in job_links:\n",
    "    #These are here to prevent a bad link or connection from messing up the entire for loop\n",
    "    try:\n",
    "        html_page = urllib2.urlopen(link).read()\n",
    "    except urllib2.HTTPError:\n",
    "        print \"HTTPError:\"\n",
    "        continue\n",
    "    except urllib2.URLError:\n",
    "        print \"URLError:\"\n",
    "        continue\n",
    "    except socket.error as error:\n",
    "        print \"Connection closed\"\n",
    "        continue\n",
    "\n",
    "    #regex rule is replace any char that is not a-z with space, carat means \"anything but\" \n",
    "    html_text = re.sub(\"[^a-z.]\",\" \", html_page.lower()) \n",
    "    for key in cities.keys():\n",
    "        if key in html_text:\n",
    "            cities[key]+=1\n",
    "            \n",
    "print cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Plot the Results\n",
    "Use a simple histogram to plot the counts using Pandas.\n",
    "Based off this, it seems clear that San Francisco is definitely the city to go to if you are a recent college graduate looking for an entry-level job as it has the largest number of jobs on the site.\n",
    "\n",
    "I also noticed the counts were really low, and after looking through the website, I realized that any search query will end up listing all jobs sorted by distance from the zip code entered, so a majority of the ~5000 links scraped were not in CA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseries = pd.Series(cities) #create pandas series from dict\n",
    "pseries.sort(ascending=False)\n",
    "\n",
    "pseries.plot(kind = 'bar') #simple histogram using pandas\n",
    "plt.title('Bay Area Cities Distribution')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/cities_dist.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
